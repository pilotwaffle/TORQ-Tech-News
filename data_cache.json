{
  "timestamp": "2025-10-22T21:38:58.435586",
  "featured": {
    "title": "Skills & Learning",
    "excerpt": "Business strategy insights from MIT Sloan School of Management",
    "image": "https://sloanreview.mit.edu/wp-content/uploads/2025/10/Kalluri-04-2400x1260-1-1200x630.jpg",
    "category": "Leadership",
    "author": "Massachusetts Institute Of Technology, About The Author",
    "date": "October 22, 2025",
    "reading_time": 10,
    "link": "https://sloanreview.mit.edu/article/whats-your-edge-rethinking-expertise-in-the-age-of-ai/",
    "slug": "whats-your-edge-rethinking-expertise-in-the-age-of-ai",
    "source": "MIT Sloan",
    "full_text": "Carolyn Geason-Beissel/MIT SMR | Getty Images\n\nSummary: When AI tools have many of the answers, what’s the value of expensive experts? It’s their ability to ask better questions and recognize gray areas, which shifts their value from content to context. Leaders should focus on developing people’s meta-expertise — their ability to orchestrate AI tools, synthesize information across domains, and make creative connections that algorithms can’t — and make space for them to take accountability, be creative, and claim some decision-making as “human only.”\n\nA CEO recently posed a question to me that’s been keeping executives awake: “If my junior analyst can get the same AI-generated insights as my senior strategist, why am I paying for expertise?”\n\nIt’s not hyperbole to say that we’re witnessing an unprecedented democratization of knowledge. Information that was once locked in specialized databases, consulting reports, and expert minds is now instantly available to anyone with access to generative AI and artificial intelligence tools. A startup founder in Indonesia can access strategic frameworks that once required McKinsey consultants. A nurse practitioner in rural Kansas can synthesize medical research like a specialist at Mayo Clinic.\n\nThis isn’t simply another wave of automation; it’s a fundamental restructuring of knowledge itself. Organizations that misunderstand this shift face two risks: overpaying for outdated expertise and undervaluing the human capabilities that remain irreplaceable.\n\nThe Paradox of Abundant Knowledge\n\nWhen knowledge becomes commoditized, its value paradoxically shifts from the content to the context. Consider three critical transformations.\n\nFrom answers to questions: AI excels at providing comprehensive answers, but only to the questions that we know to ask. The most valuable human expertise increasingly lies in identifying unasked questions and recognizing that there are unknown unknowns. A seasoned strategist understands not only their industry’s current patterns but also its hidden assumptions and unexplored adjacencies — the white spaces that don’t yet exist in any AI model’s training data.\n\nAI excels at providing comprehensive answers, but only to the questions that we know to ask. The most valuable human expertise increasingly lies in identifying unasked questions and recognizing that there are unknown unknowns. A seasoned strategist understands not only their industry’s current patterns but also its hidden assumptions and unexplored adjacencies — the white spaces that don’t yet exist in any AI model’s training data. From information to judgment: While AI can instantly synthesize vast amounts of information, it cannot bear the weight of consequences. When an AI system recommends restructuring your organization’s supply chain or entering a new market, the accountability remains entirely human. This gap between intelligence and responsibility creates an irreplaceable role for human judgment. Leaders aren’t paid because they can access information; they’re paid to make decisions when the stakes are real and the outcomes are uncertain.\n\nWhile AI can instantly synthesize vast amounts of information, it cannot bear the weight of consequences. When an AI system recommends restructuring your organization’s supply chain or entering a new market, the accountability remains entirely human. This gap between intelligence and responsibility creates an irreplaceable role for human judgment. Leaders aren’t paid because they can access information; they’re paid to make decisions when the stakes are real and the outcomes are uncertain. From static knowledge to liquid knowledge: Traditional knowledge management has treated information as a fixed asset to be stored and retrieved from knowledge repositories. But AI reveals knowledge dynamically, reshaping it based on the context, user, and moment. Each prompt generates a unique knowledge artifact tailored to specific needs. This shift from static knowledge to liquid knowledge fundamentally changes how organizations should think about subject matter expertise.\n\nAbout the Author Ravikiran Kalluri is an assistant teaching professor at Northeastern University.\n\nReferences 1. A.R. Doshi and O.P. Hauser, “Generative AI Enhances Individual Creativity but Reduces the Collective Diversity of Novel Content,” Science Advances 10, no. 28 (July 12, 2024): 1-9, https://doi.org/10.1126/sciadv.adn5290. 2. H.-P. Lee, A. Sarkar, L. Tankelevitch, et al., “The Impact of GenAI on Critical Thinking: Self-Reported Reductions in Cognitive Effort and Confidence Effects From a Survey of Knowledge Workers,” in “CHI ’25: Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems” (Association for Computing Machinery, 2025): 1-22, https://doi.org/10.1145/3706598.3713778. 3. A.S. George, T. Baskar, and P.B. Srikaanth. “The Erosion of Cognitive Skills in the Technological Age: How Reliance on Technology Impacts Critical Thinking, Problem-Solving, and Creativity,” Partners Universal Innovative Research Publication 2, no. 3 (May-June 2024): 147-163, https://doi.org/10.5281/zenodo.11671150. 4. H. Mudassir, K. Munir, S. Ansari, et al., “AI Can (Mostly) Outperform Human CEOs,” Harvard Business Review, Sept. 26, 2024, https://hbr.org.\n\nReprint #: 67223\n\nThe most valuable human expertise increasingly lies in identifying unasked questions and recognizing that there are unknown unknowns.[",
    "summary": "Business strategy insights from MIT Sloan School of Management",
    "keywords": [
      "human",
      "synthesize",
      "expertise",
      "paid",
      "ai",
      "age",
      "edge",
      "rethinking",
      "knowledge",
      "information",
      "answers",
      "organizations",
      "questions",
      "whats"
    ],
    "author_title": "Tech Analyst"
  },
  "articles": [
    {
      "title": "Skills & Learning",
      "excerpt": "Business strategy insights from MIT Sloan School of Management",
      "image": "https://sloanreview.mit.edu/wp-content/uploads/2025/10/Kalluri-04-2400x1260-1-1200x630.jpg",
      "category": "Leadership",
      "author": "Massachusetts Institute Of Technology, About The Author",
      "date": "October 22, 2025",
      "reading_time": 10,
      "link": "https://sloanreview.mit.edu/article/whats-your-edge-rethinking-expertise-in-the-age-of-ai/",
      "slug": "whats-your-edge-rethinking-expertise-in-the-age-of-ai",
      "source": "MIT Sloan",
      "full_text": "Carolyn Geason-Beissel/MIT SMR | Getty Images\n\nSummary: When AI tools have many of the answers, what’s the value of expensive experts? It’s their ability to ask better questions and recognize gray areas, which shifts their value from content to context. Leaders should focus on developing people’s meta-expertise — their ability to orchestrate AI tools, synthesize information across domains, and make creative connections that algorithms can’t — and make space for them to take accountability, be creative, and claim some decision-making as “human only.”\n\nA CEO recently posed a question to me that’s been keeping executives awake: “If my junior analyst can get the same AI-generated insights as my senior strategist, why am I paying for expertise?”\n\nIt’s not hyperbole to say that we’re witnessing an unprecedented democratization of knowledge. Information that was once locked in specialized databases, consulting reports, and expert minds is now instantly available to anyone with access to generative AI and artificial intelligence tools. A startup founder in Indonesia can access strategic frameworks that once required McKinsey consultants. A nurse practitioner in rural Kansas can synthesize medical research like a specialist at Mayo Clinic.\n\nThis isn’t simply another wave of automation; it’s a fundamental restructuring of knowledge itself. Organizations that misunderstand this shift face two risks: overpaying for outdated expertise and undervaluing the human capabilities that remain irreplaceable.\n\nThe Paradox of Abundant Knowledge\n\nWhen knowledge becomes commoditized, its value paradoxically shifts from the content to the context. Consider three critical transformations.\n\nFrom answers to questions: AI excels at providing comprehensive answers, but only to the questions that we know to ask. The most valuable human expertise increasingly lies in identifying unasked questions and recognizing that there are unknown unknowns. A seasoned strategist understands not only their industry’s current patterns but also its hidden assumptions and unexplored adjacencies — the white spaces that don’t yet exist in any AI model’s training data.\n\nAI excels at providing comprehensive answers, but only to the questions that we know to ask. The most valuable human expertise increasingly lies in identifying unasked questions and recognizing that there are unknown unknowns. A seasoned strategist understands not only their industry’s current patterns but also its hidden assumptions and unexplored adjacencies — the white spaces that don’t yet exist in any AI model’s training data. From information to judgment: While AI can instantly synthesize vast amounts of information, it cannot bear the weight of consequences. When an AI system recommends restructuring your organization’s supply chain or entering a new market, the accountability remains entirely human. This gap between intelligence and responsibility creates an irreplaceable role for human judgment. Leaders aren’t paid because they can access information; they’re paid to make decisions when the stakes are real and the outcomes are uncertain.\n\nWhile AI can instantly synthesize vast amounts of information, it cannot bear the weight of consequences. When an AI system recommends restructuring your organization’s supply chain or entering a new market, the accountability remains entirely human. This gap between intelligence and responsibility creates an irreplaceable role for human judgment. Leaders aren’t paid because they can access information; they’re paid to make decisions when the stakes are real and the outcomes are uncertain. From static knowledge to liquid knowledge: Traditional knowledge management has treated information as a fixed asset to be stored and retrieved from knowledge repositories. But AI reveals knowledge dynamically, reshaping it based on the context, user, and moment. Each prompt generates a unique knowledge artifact tailored to specific needs. This shift from static knowledge to liquid knowledge fundamentally changes how organizations should think about subject matter expertise.\n\nAbout the Author Ravikiran Kalluri is an assistant teaching professor at Northeastern University.\n\nReferences 1. A.R. Doshi and O.P. Hauser, “Generative AI Enhances Individual Creativity but Reduces the Collective Diversity of Novel Content,” Science Advances 10, no. 28 (July 12, 2024): 1-9, https://doi.org/10.1126/sciadv.adn5290. 2. H.-P. Lee, A. Sarkar, L. Tankelevitch, et al., “The Impact of GenAI on Critical Thinking: Self-Reported Reductions in Cognitive Effort and Confidence Effects From a Survey of Knowledge Workers,” in “CHI ’25: Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems” (Association for Computing Machinery, 2025): 1-22, https://doi.org/10.1145/3706598.3713778. 3. A.S. George, T. Baskar, and P.B. Srikaanth. “The Erosion of Cognitive Skills in the Technological Age: How Reliance on Technology Impacts Critical Thinking, Problem-Solving, and Creativity,” Partners Universal Innovative Research Publication 2, no. 3 (May-June 2024): 147-163, https://doi.org/10.5281/zenodo.11671150. 4. H. Mudassir, K. Munir, S. Ansari, et al., “AI Can (Mostly) Outperform Human CEOs,” Harvard Business Review, Sept. 26, 2024, https://hbr.org.\n\nReprint #: 67223\n\nThe most valuable human expertise increasingly lies in identifying unasked questions and recognizing that there are unknown unknowns.[",
      "summary": "Business strategy insights from MIT Sloan School of Management",
      "keywords": [
        "human",
        "synthesize",
        "expertise",
        "paid",
        "ai",
        "age",
        "edge",
        "rethinking",
        "knowledge",
        "information",
        "answers",
        "organizations",
        "questions",
        "whats"
      ],
      "author_title": "Tech Analyst"
    },
    {
      "title": "AI & Machine Learning",
      "excerpt": "Business strategy insights from MIT Sloan School of Management",
      "image": "https://sloanreview.mit.edu/wp-content/uploads/2025/10/Pozen-2400x1260-1-1200x630.jpg",
      "category": "Leadership",
      "author": "Massachusetts Institute Of Technology, About The Authors",
      "date": "October 22, 2025",
      "reading_time": 7,
      "link": "https://sloanreview.mit.edu/article/for-ai-productivity-gains-let-team-leaders-write-the-rules/",
      "slug": "for-ai-productivity-gains-let-team-leaders-write-the-rules",
      "source": "MIT Sloan",
      "full_text": "Carolyn Geason-Beissel/MIT SMR | Getty Images\n\nSummary: Companies are pouring money and effort into AI while overlooking a simple truth about adoption and governance: The real work happens at the team level. A recent survey of 348 business leaders reveals a critical gap between usage rules and realities. Seventy-two percent say corporate leadership should establish overall AI guidelines while individual teams set their own rules. Team leaders understand local context, judgment calls, and daily risks. So why aren’t you delegating more power to them?\n\nCorporations are rushing to invest in artificial intelligence. But corporatewide AI policies alone can’t transform work, according to our recent research. For AI efforts to pay off, executives must set clear guardrails while enabling teams to write the rules that make tool adoption real. Only then will AI deliver the meaningful returns that organizational leaders are pursuing.\n\nIn the productivity classes one of us (Robert) teaches at MIT, many executives are keen to learn about AI but unsure about how current corporate rules would let them use the new technology. This led to the design of a survey, taken by 348 business professionals — including senior executives, line managers, and team members from various industries — about who gets to make AI rules and how AI rules actually function inside their companies. The findings are clear-cut: Businesses have been overlooking the team dimensions of AI governance and implementation.\n\nA striking 72% of the participants in the survey said that corporate headquarters should establish overall AI guidelines, but individual teams must set their own rules within those boundaries.\n\nThe Trouble With Centralization\n\nSome organizations have responded to the AI craze by creating AI czars or centers of excellence, issuing corporate mandates, or establishing executive task forces. While these moves may feel decisive, they seldom change how real work gets done.\n\nCentralization tends to clog the arteries. Teams wait for approval, workarounds proliferate, and some employees play it safe by circumventing AI productivity tools instead of harnessing them.\n\nWe’ve seen the challenges of new productivity tools before. However, the arrival of the internet didn’t lead to “internet departments,” and no one ever asked permission to open a spreadsheet. AI should be governed the same way: with principled guardrails at the corporate level and practical rules created at the team level.\n\nAI tools have the capability to write, summarize, and analyze with speed. But AI cannot grasp context, apply judgment, or eliminate error. Responsibilities vary greatly by function, of course: The finance department is different from customer service or site operations. Because judgment is local, rules must be local and set by team leaders.",
      "summary": "Business strategy insights from MIT Sloan School of Management",
      "keywords": [
        "tools",
        "ai",
        "write",
        "let",
        "leaders",
        "rules",
        "work",
        "teams",
        "productivity",
        "gains",
        "survey",
        "set",
        "team",
        "corporate"
      ]
    },
    {
      "title": "Marketing Strategy",
      "excerpt": "Business strategy insights from MIT Sloan School of Management",
      "image": "https://sloanreview.mit.edu/wp-content/uploads/2025/07/Webinar-Acar-2400x1260-1-1200x630.jpg",
      "category": "Leadership",
      "author": "Massachusetts Institute Of Technology, About The Authors",
      "date": "October 22, 2025",
      "reading_time": 11,
      "link": "https://sloanreview.mit.edu/video/reimagining-marketing-strategy-for-the-ai-era/",
      "slug": "marketing-strategy",
      "source": "MIT Sloan",
      "full_text": "Learn how to strategically implement generative AI in marketing while managing risks and building consumer trust.\n\nMany marketing leaders recognize the opportunity of using generative AI for marketing but struggle with a fundamental question: Where to begin? The challenge isn’t whether to adopt generative AI but how to systematically identify which marketing tasks offer the highest opportunities with the most manageable risks.\n\nJoin professor Oguz A. Acar from King’s College London, a leading expert in AI and marketing innovation, as he provides a strategic road map for reimagining your marketing approach in the AI era. Drawing from cutting-edge research and real-world case studies, he’ll help you identify high-impact opportunities while avoiding costly pitfalls.\n\nIn this webinar, you will learn:\n\nHow to identify and prioritize the most promising GenAI applications for your marketing organization.\n\nStrategies to build consumer trust when deploying AI-powered marketing initiatives.\n\nKey considerations in data privacy and ethics in AI marketing.\n\nHow AI agents are reshaping brand-consumer relationships and what this means for your customer engagement strategy.\n\nPractical lessons from companies that have successfully implemented GenAI marketing programs — and those that haven’t.\n\nCost-benefit analysis approaches for evaluating AI marketing investments.",
      "summary": "Business strategy insights from MIT Sloan School of Management",
      "keywords": [
        "marketing",
        "ai",
        "consumer",
        "webinar",
        "genai",
        "using",
        "strategy",
        "identify",
        "generative",
        "reimagining",
        "era",
        "trustmany",
        "opportunities"
      ]
    },
    {
      "title": "AI & Machine Learning",
      "excerpt": "Business strategy insights from MIT Sloan School of Management",
      "image": "https://sloanreview.mit.edu/wp-content/uploads/2025/10/MMAI-S12-E3-Patel-Cisco-headshot-2400x1260-1-1200x630.jpg",
      "category": "Leadership",
      "author": "Massachusetts Institute Of Technology, About The Host",
      "date": "October 22, 2025",
      "reading_time": 7,
      "link": "https://sloanreview.mit.edu/audio/never-fight-a-megatrend-ciscos-jeetu-patel/",
      "slug": "ai-machine-learning",
      "source": "MIT Sloan",
      "full_text": "Cisco is well known for its data, networking, security, and collaboration products. On today’s episode of the Me, Myself, and AI podcast, Cisco’s president and chief product officer, Jeetu Patel, joins host Sam Ransbotham for a discussion about artificial intelligence, a “megatrend” Jeetu sees as perhaps more significant than the development of the internet or the automobile because of its ability to build on past technological advances.\n\nJeetu and Sam discuss how to manage AI and how to staff for it — Jeetu argues that replacing less experienced or younger workers with technology deprives organizations of key perspectives and new ideas, and instead advocates for developing reverse-mentoring programs inside organizations.\n\nJeetu Patel, Cisco Jeetu Patel, Cisco’s president and chief product officer, combines product design and development expertise, operational rigor, and market understanding to create high-growth businesses. He is tasked with building world-class products to solve customers’ problems and connect and protect every aspect of their organization in the AI era. Previously a general manager at Cisco, he led the strategy and development of its Security and Collaboration businesses. Before Cisco, Patel was the chief product officer and chief strategy officer at cloud content management company Box. He’s also held roles at EMC, including chief executive of its Syncplicity business unit, CMO for the Information Intelligence Group, and chief strategy officer. He currently serves on the board of JLL, a commercial real estate services company. Jeetu has a bachelor’s degree in information decision sciences from the University of Illinois at Chicago and lives in the San Francisco Bay Area.\n\nSubscribe to Me, Myself, and AI on Apple Podcasts or Spotify.\n\nTranscript\n\nAllison Ryder: We talk a lot about AI being overhyped. Today’s guest believes … it’s not. Continue listening to learn how his point is connected to tech infrastructure and security.\n\nJeetu Patel: I’m Jeetu Patel from Cisco. And you’re listening to Me, Myself, and AI .\n\nSam Ransbotham: Welcome to Me, Myself, and AI , a podcast from MIT Sloan Management Review exploring the future of artificial intelligence. I’m Sam Ransbotham, professor of analytics at Boston College. I’ve been researching data, analytics, and AI at MIT SMR since 2014, with research articles, annual industry reports, case studies, and now 12 seasons of podcast episodes. In each episode, corporate leaders, cutting-edge researchers, and AI policy makers join us to break down what separates AI hype from AI success.\n\nHi, listeners. Thanks for joining us again. Today, I am talking with Jeetu Patel, president and chief product officer at Cisco. Jeetu, thanks for joining us.\n\nJeetu Patel: Thank you for having me, Sam. It’s great to be here.\n\nSam Ransbotham: As we talk, Cisco equipment is probably behind 90% of the infrastructure that we’re using, but some listeners may not be aware of all that Cisco does. So let’s start off there. Jeetu, can you give us some background on Cisco and, in particular, your role?\n\nJeetu Patel: Sure. I’ll start in reverse order. I run products at Cisco. So all the products that you use from Cisco, whether it be networking products, whether it be security products, whether it be data products, or collaboration products, those typically are ones that I’m in charge of building and taking to market, of course with a very, very capable team. Essentially, the way you should think about Cisco is we are the critical infrastructure company for the AI era. So all of the plumbing that’s required to make sure that people can connect, [that] people can stay secure while they’re connected, and that people can make sure that they have the data platform, those are the things that we provide to the market.\n\nSam Ransbotham: I saw a quote from you saying, “Think of us as the picks and shovels [company] during the AI gold rush era.”\n\nJeetu Patel: Right.\n\nSam Ransbotham: For listeners who may not be aware, there’s a famous — or infamous —statement that the people who made money in the gold rush were people who sold picks and shovels to the individual miners. Some of them made it big, but some of them did not. And I think your analogy is quite spot on there.\n\nJeetu Patel: One of the things that you find when you go through these kinds of massive — what I would call disruptive — platform shifts, where we’ve all been going down a certain path with a certain set of assumptions, and then the assumptions change because a whole new set of technologies emerge, which is what AI is, what you find is the infrastructure that’s required to go out and run those technologies needs to be rethought and reimagined. And anytime there’s any one of these major platform shifts, the infrastructure providers tend to make out pretty well because you have to change the entire plumbing of the apparatus that’s going to be used. So if you think about when automobiles were built, you now need to have roads, and you need to have expressways, and you need to have traffic lights, and you need to have a whole system in place for how automobiles actually get integrated into society.\n\nIt’s not just the automobile, but everything around it needs to change. If you think about AI right now, these data centers where these digital workers are going to live, they have to be completely reimagined, because the current data center does not have the power availability and the compute requirements and the network bandwidth to be able to fulfill and satiate the needs of what an AI system would need. So you have to kind of rethink and reimagine and, as they call it in technical terms, rerack the data centers because there [are] racks and racks of computers, and network and switching gear that actually have to be cooled in a certain way, and so on and so forth. That entire shift is what we are in the midst of. And Cisco is a natural benefactor of it because we provide the infrastructure for the AI era.\n\nSam Ransbotham: [Compared to] some of the analogies you’re making — we talked about the gold rush, we talked about the internet, we talked about cars — is artificial intelligence at that level of big deal, or not?\n\nJeetu Patel: I think it might be bigger. The way I’ve seen these kinds of shifts happen: Imagine if Amazon.com got built in the 1600s. It would be an epically failed company, because you didn’t have the internet and you didn’t have the underlying infrastructure on top of which Amazon could be built, because you didn’t have the shipping and the logistics infrastructure and the internet and all of those pieces. In a similar vein with AI, we have the benefit of having all of the infrastructure that’s been built out to date. When you have something that’s built taking advantage of all of these things, by definition, each one of these subsequent, major, transformative waves tends to be bigger in impact than the previous ones, just because it was built on top of shoulders of giants of the previous innovations that had happened.\n\nSo I would say this is probably the most consequential set of inventions that we will have seen in our lifetime, for sure, and probably arguably in humanity. The thing to keep in mind is the pace of innovation and the slope at which it happens actually makes it impossible to predict what’s going to happen three years from now because we’ve compressed the time. Scientific progress will probably compound by 1,000x. And so you’ve compressed this timescale. … We’re in a warped situation right now where humans can’t make sense of this because everything’s moving so fast.\n\nSam Ransbotham: Actually, there’s so many things to talk about there. I think your analogy to Amazon and the 1600s is interesting because, for example, we had neural network designs back in the 1980s. We just didn’t have the compute infrastructure, the data, the telecom to pull it off. So it took all these things coming together. And I think what you’re saying is that all these things are together now for us to build from.\n\nJeetu Patel: Actually, the biggest thing that we have that we didn’t have then, because this would’ve been hard to go invent, is AI has been around for a while, but when did it actually take off? It took off on Nov. 30, 2022. What was so significant about that date? That’s when ChatGPT was launched. What was so significant about ChatGPT? It was essentially what they call a large language model. And a large language model was a model that actually understood human language, rather than the machine having to be rigid and the human having to learn the machine’s language.\n\nIt became the other way around. How did that happen? That happened because we had petabytes and petabytes of publicly available data on the internet that you could use to train these models so that these models would then know what to do with it. So if you didn’t have the internet, you would’ve not had AI because you would’ve not had that level of data to then train the models on. It goes back to our point of each one of these inventions or revolutions is built on the infrastructure provided by the previous revolution. And it’s very evident in this case.\n\nSam Ransbotham: One of the last conversations that I had with my grandfather was how much life had changed from no electricity, limited indoor plumbing, no airplanes, no space travel, no computers, to when he passed — how radically different that was. And I remember talking with him at the time, saying, “Oh, wow, you lived through a bunch. I can’t imagine things changing as much during my lifetime.” I may very quickly be eating those words.\n\nJeetu Patel: I think we get into this kind of cycle. … Humans are not very good, in general, at imagining the exponential outcomes. We’re very good at imagining linear regression but not the exponentiality of the outcome. So what ends up happening is we think about exponentiality in a single dimension, not multidimensional. … Years ago, I had a chance to sit down and talk to Ray Kurzweil, who’s one of the scientists at Google. This was 20 years ago or something. And I was interviewing him for something, and we were talking about this notion of perpetual extension of life: Can a human live long enough to live forever? He had written this book where his thesis at the time was [that] if you live until you’re 40, we will have the science and technology to allow us to live in perpetuity.\n\nMy topic of communication with him was around the social implications of that. What happens if seven generations live simultaneously or 10 generations live simultaneously? That’s going to be really hard because we’re not going to have enough room to put everyone, and we’re not going to have enough crops to go feed everyone. And he’s like, “You know, this is the problem with humans. … We can’t think in exponential terms because we think in a single dimension of exponentiality,” which is, if seven generations or 10 generations lived simultaneously, what would happen with everything else being exactly the same? But the reality is you might have skyscrapers that might be 2,000 stories high, and you might have a crop cycle that takes three days.\n\nAnd so those are all things that would also simultaneously evolve so that they can accommodate the constraints that get created because of the developments that happen in certain areas. I feel like that’s the same over here, when people say, “You know, I think humans are going to be sitting on a beach, have nothing to do, and AI is going to do everything,” I just chuckle a little bit because I just refuse to believe that humans are designed to be obsolete.\n\nSo we will continue to find ways to add value and think creatively. That doesn’t mean that we’ll be doing what we do today. It might very well mean that all the jobs that we do today might not be the jobs that we have, but that also doesn’t mean that we’re not going to have jobs. … The desire for a human to be productive and add value to society doesn’t go away because something got automated. You just create higher-order bits that you’ll then be able to go focus on that you were not able to focus on in the past.\n\nSam Ransbotham: Exactly. Just to make sure that I can get this thrown back in my face later, I’m predicting massive increases [in] employment, not decreases. No one since the internet is doing less than they were before the internet, despite all the progress possible. I can’t believe that’s not going to happen again. I think we’re headed toward the opposite direction.\n\nJeetu Patel: I think you’ll see some displacement of jobs temporarily, which we should not take lightly, because I think it’ll cause human suffering. But that does not mean that that will be the state in perpetuity. What you have to keep in mind is that displacement period, if we get ahead of it, because of the pattern that we’re starting to witness, we might be able to actually get society retrained in a more efficient way than we might have done in the past with previous disruptions. And that might be a responsibility that the tech community, the collaboration between the public and private sector, should have as a good, beneficial outcome, because I think there’s going to be more and more of a need for collaboration across different sectors.\n\nThis is one of those areas where I tend to be, in the long term, an optimist without being naive about the short-term implications that this might have. Even [the] mid-term implications it has around safety and security, and the downside effects could be profoundly consequential that we have to keep in mind. But I refuse to believe that we’re going to be obsolete or that we’re not going to have value to add. It just seems unnatural.\n\nSam Ransbotham: I think that’s well put, that we can have a positive aggregate effect but still have lots of heterogeneity in how that average plays out across society, and being naive about that is going to hurt us in the long run. You mentioned security, though. And when we were talking about reracking and infrastructure changes, we quickly slipped toward routers and modems and telecom and hardware-oriented things. But one of the things that I think you’re very focused on in terms of infrastructure is the idea of security, and how does that become a first-class player versus the thing that’s derided as hampering productivity.\n\nSo we’ve always had this sort of productivity-security trade-off. I think that you’ve mentioned that we may not be making that trade-off anymore. How can we help security be part of the infrastructure?\n\nJeetu Patel: Firstly, I think in this particular age, security is going to be a prerequisite for successful adoption of AI because if people don’t trust these systems, they’re not going to use them. That’s very different from in the past, where you would think about security as a necessary productivity impediment. That’s no longer the case. Now it just happens to be a prerequisite for successful adoption of AI.\n\nBut I actually feel like it’s probably worth taking a step back and saying, “Where are we still thinking very linearly?” Where I feel AI is underhyped the most is the fact that we still keep thinking that this is just a productivity game. Humans are going to get more productive. Things are going to happen cheaper, faster, better. I actually feel that’s only the first-order effect.\n\nThe second-order effect is [that] you will actually start to see these AI models. It’s not even clear if large language models will be the ultimate destination. You’ll have large world models. You’ll have physical models. All of these things will start kind of combining together. But the new paradigm, whatever it ends up being at some point in time, and the existing ones will start to create original insight that did not exist in the human corpus of knowledge. It won’t just be an aggregation mechanism. It won’t just be where you take a multitude of different perspectives. This is not just a better search engine, where humans had some data, and you indexed that data well, and you were able to go out and put it into a clean paragraph. It’s going to start creating original insights that didn’t exist in the human corpus of knowledge.\n\nAnd when that happens, the thing that changes is [that] you are now able to imagine solving problems that you could never even dream of solving before. And that is far beyond just going out and optimizing for productivity. I feel like that’s the most misunderstood part of AI: “Oh, I’m just going to get more productive.” Productive use is going to be like 10% of the equation. Going out and doing things you couldn’t do before, and solving problems you couldn’t solve before in different ways that you couldn’t even dream of solving before, is probably going to be the 90% factor.\n\nSam Ransbotham: I like that idea, to observe the productivity effects first, because, as you say, they’re first order, but how do companies, how do people start thinking about what they can do with that 90%? Productivity is pretty tempting. I mean, I like greater productivity. This can be hard for me to turn away from. Or maybe turning away is me sort of making it a Hobson’s choice, where you have to do one or the other. But how do people start thinking about this 90% or these more than productivity options? I don’t think I’ve ever talked to anybody who thought AI was underhyped. And I think we’re on record for saying that here.\n\nJeetu Patel: I think where it’s overhyped is where the human obsolescence becomes almost a foregone conclusion in some people’s minds. I think that’s where it’s overhyped. I feel like human instinct and human judgment [are] still pretty hard to go out and replicate in the machine’s ability to do things because we don’t make most of our decisions based on data. We make a lot of our decisions based on gut. And that gut is hard to go replicate. Typically, people say, “Listen to your gut,” because there’s a reason for it. There’s an instinct that’s palpable.\n\nBut to go back to your question of what should companies be thinking about. By the way, I think I highly encourage the productivity argument. I think everyone should go out and think about productivity. And they should continue to keep kind of powering through that. It’s going to be a great benefit that we will all be recipients of. Where I think the unlock truly comes in is by actually trying to make sure that we challenge the conventional norms of thinking and ask ourselves the questions, “What problems have we been conditioned to think that we can’t solve?” and “Are those unsolvable moving forward just the way that they were in the past?” I feel like you’ll actually start to find very different answers.\n\nI feel like we need more and more questioning … of the status quo. And the way to do that, in my mind, the one single thing is going to be the exponential difference. You have to unlearn as much as you are learning. Unlearning requires that you actually inject new talent into the system at a very rapid pace, and then give creative freedom to that talent so that you are actually getting mentored by them just as much as you’re mentoring them.\n\nThis was a recent conversation I’d had at a conference I was at, where people were like, “Entry-level jobs are going to go away,” and “We’re just not going to hire early-career people,” which in my mind seems like the stupidest idea that a company could pursue. If you actually don’t hire new people to come in, you have essentially given up on [the] injection of new talent and new ideas into the thought process. So this kind of baggage of experience will always hold you back. You know a lot of things, and you might not be, as a company, good at unlearning, and you have no one else to actually instigate that unlearning and catalyze that unlearning by asking questions, because they didn’t have the baggage of knowing. I do feel like the mix, the continued infusion of talent [that’s] early in career is going to be so important for companies to be able to get the most out of it, because we have to understand instinctively how to go out and use these tools, which are in service of humans, in a very different way than the way that we might’ve used them in the past.\n\nAnd right now, frankly, if you take someone who is a 20-year-old and a 28-year-old, and compare the two of them and how they use AI, it’s night-and-day different.\n\nA 28-year-old might actually use it for productivity. They’ll go out and they’ll ask it some questions because they’ve got some answers to get. Then they’ll move on. A 20-year-old thinks of it like a companion and a brainstorming partner. And they might actually talk to it and brainstorm with it so that they can come to an ideation. They’re not looking for answers. They’re looking for substantive volleying back and forth, and brainstorming. I learned that from the interns that would come into Cisco. So I think that’s the aspect that I think we have to keep in mind, that as a company, we have to make sure that we keep challenging and disrupting ourselves before someone else disrupts us.\n\nBy the way, innovation is not limited. This is the one thing that people kind of bucket into these completely unproductive ways, which is you’re a small startup, you innovate really fast, you become large, you stop innovating. I think it’s nonsense, because innovation is not like something that’s limited to a certain group of people. Anyone can choose to innovate at any point in time. You just have to have the right mental model and mindset. What you have to do is fight the temptation for bureaucracy being something that you succumb to. So challenge the bureaucracy and allow people to come in [who] challenge the status quo, and … by definition, the byproduct of that is going to be invention.\n\nSam Ransbotham: The unlearning idea really hits home for me. I think we’re going to have to cut that from the episode because I don’t want my kids to hear it and then think that I am not full of wisdom and that their ideas are important. But it appeals to me, because many of the things that I think that got me to where I am in my career are not necessarily the things that seem like they’re going to keep me going through the next phase of my life. And so that unlearning makes sense, but at the same time, I have trouble knowing what things I should unlearn and what things I shouldn’t unlearn, and I feel like companies have that same problem. They got successful through some strategic core competency. The idea that “Oh, yeah, we unlearned everything” seems too global to me. So how do we decide?\n\nJeetu Patel: No, I don’t think you have to unlearn everything. I think humans build on top of each other’s learnings. One of the most important inventions that ever was created was a printing press because we were able to communicate the learning from one generation to the other in a way that was very concrete. The combination of language, script, and the printing press, and [the] ultimate level of desire to share your knowledge with others, which is instinctive to us, and [the] ultimate desire to learn from other people’s learnings, which is also instinctive to us, was actually pretty valuable.\n\nSo I don’t believe that you should unlearn everything, but I believe that pairing up experience with inexperience is really valuable. And … make sure that there’s a bidirectional mentorship that’s occurring in your ethos of your organization, where the experienced people are coaching the early entrants — by the way, by early entrants, I don’t always mean young people, because I could be inexperienced in a brand-new domain. Sometimes I have to force myself to just go into uncomfortable spots and go into new domains where I can learn [a] new thing, that I can ask questions that might not be conventional wisdom.\n\nWhat I think is really important is conventional wisdom helps many times. And many times, conventional wisdom prevents us from exploring something that we want to explore, and thereby creates barriers that are unnecessary. And so that’s what we have to kind of undo. That’s the area that I feel like there’s opportunity for organizations thinking differently. Don’t just mentor your interns. Have a reverse mentorship program as well. So if you’re spending an hour with an intern, make sure that one of your key objectives is — for 30 minutes of it — make sure you’re getting something out of it, not just them. It’s not one-directional. It’s bidirectional.\n\nI think I’m practicing what I’m preaching in this conversation that I had with you because I’ve tried to always go into areas that I knew nothing about. And I found that keeps me curious, that keeps me motivated, that keeps me learning. And it also allows me the permission to ask silly questions, which then free me from the burden of experience sometimes that I have to have in certain areas. And then in other areas, [the] number of years in the system teaches you patterns that as long as you don’t think those patterns are non-shiftable, then you actually benefit from them. I always think [that] “strong opinions, loosely held” is a good model, which is completely flippable with new arguments and new data.\n\nThe reason I got into computers was because some uncle told me the night before that it seems … better than going into business management. So I took computers as a class. And then I got into that and got interested. And from there, [I got] into consulting. And then I did consulting for a long time. I started my own business. And then the consulting thing got boring after a while because I didn’t feel like I had exponential scale in that business. And I wanted to learn scale because I was fascinated with scale. So I got into software.\n\nFrom software, one thing led to the other, and I got into products. And products were kind of fun to use because you started doing things in the cloud. And then the cloud became fun because … now you’re starting to benefit from AI. The formula that I’ve used is “don’t ever fight a megatrend, and use it as a tailwind.”\n\nAnd know the difference between a megatrend and a hype cycle. So if you can go out and effectively deduce what is a megatrend [and] what is the hype cycle, not fight the megatrend and ignore the hype cycle, you have an advantageous position in society. Now, by the way, the instructive question is, How do you know the difference? In my mind, there’s a simple formula, which is if it requires a Ph.D. for someone to explain what the benefit of something is, it is a hype cycle. If it’s something that is instantly obvious in what the benefit could be, where you could imagine five steps forward, it is likely a megatrend.\n\nSam Ransbotham: We’ve had a very conversational-oriented [discussion] so far. Let me switch to … rapid-fire questions. Just give me the first thing that comes off your mind.\n\nWhat’s moving faster about artificial intelligence or slower than you expected?\n\nJeetu Patel: What’s moving faster is the rate of change. And what’s moving slower is the use cases that organizations are actually starting to find tangible value from. I think the technology is moving fast. The adoption is moving slower.\n\nSam Ransbotham: What’s been the best use of artificial intelligence so far for you personally?\n\nJeetu Patel: Research. Getting dexterous in a particular domain in a fraction of the time of what it could be in the past is something that I don’t think I would’ve been able to do [in] my job currently and have taken that job on and been able to get up to speed as fast if it weren’t for AI. I am a direct benefactor of AI. My family would not be fed the way it is today if AI wasn’t around. It’s that simple.\n\nSam Ransbotham: What do you wish that AI could do better? Or what frustrates you about AI?\n\nJeetu Patel: I think we are still in a very kind of chat-based interface. Yet I think we are squarely entering into the next phase, which is agents being able to conduct tasks and jobs fully autonomously. I still do a lot of things I hate doing in my day that I think at some point in time AI will take off my plate. I don’t think we’re quite there yet.\n\nSam Ransbotham: Amen to that. Has using artificial intelligence made you spend more time with technology or less?\n\nJeetu Patel: More.\n\nSam Ransbotham: More?\n\nJeetu Patel: Because I’m just curious. I spend from 9 to midnight every night, almost, in my learning mode, which is something I never really did quite that religiously before AI.\n\nSam Ransbotham: Well, it’s been fascinating talking with you and learning. … I like the phrase about never fighting a megatrend. And if you’re right about AI being underhyped, which I think you’ve made some cogent arguments for, then we’re in the middle of a real shift. Thanks for taking the time to talk with us today.\n\nJeetu Patel: Thank you for having me.\n\nSam Ransbotham: I hope you enjoyed the conversation today. In two weeks, I’ll be joined by Kathleen Peters, chief innovation officer at Experian. Please tune in.\n\nAllison Ryder: Thanks for listening to Me, Myself, and AI . Our show is able to continue, in large part, due to listener support. Your streams and downloads make a big difference. If you have a moment, please consider leaving us an Apple Podcasts review or a rating on Spotify. And share our show with others you think might find it interesting and helpful.",
      "summary": "Business strategy insights from MIT Sloan School of Management",
      "keywords": [
        "jeetu",
        "actually",
        "things",
        "ai",
        "think",
        "way",
        "ciscos",
        "megatrend",
        "thats",
        "ransbotham",
        "fight",
        "going",
        "patel",
        "cisco"
      ]
    },
    {
      "title": "Ovi: Twin backbone cross-modal fusion for audio-video generation",
      "excerpt": "Popular on Hacker News with 241 points",
      "image": "https://opengraph.githubassets.com/0e5ffc9f2d338c475698b5f15e1325ab393bc8e6326e4bb4ce4ea14a5c6ab20a/character-ai/Ovi",
      "category": "Technology",
      "author": "montyanderson",
      "date": "October 22, 2025",
      "reading_time": 9,
      "link": "https://github.com/character-ai/Ovi",
      "source": "Hacker News",
      "full_text": "Video Demo\n\nfinal_ovi_trailer.mp4\n\n🌟 Key Features\n\nOvi is a veo-3 like, video+audio generation model that simultaneously generates both video and audio content from text or text+image inputs.\n\n🎬 Video+Audio Generation : Generate synchronized video and audio content simultaneously 🎵 High-Quality Audio Branch : We designed and pretrained our 5B audio branch from scratch using our high quality in-house audio datasets\n\n: Generate synchronized video and audio content simultaneously 📝 Flexible Input : Supports text-only or text+image conditioning\n\n: Supports text-only or text+image conditioning ⏱️ 5-second Videos : Generates 5-second videos at 24 FPS, area of 720×720, at various aspect ratios (9:16, 16:9, 1:1, etc) 🎯 High-Resolution Support : Feel free to try 960×960 area (e.g., 720×1280, 704×1344, etc) - it could give outstanding results for both t2v and i2v! See examples below:\n\n: Generates 5-second videos at 24 FPS, area of 720×720, at various aspect ratios (9:16, 16:9, 1:1, etc) 🎬 Create videos now on wavespeed.ai : https://wavespeed.ai/models/character-ai/ovi/image-to-video & https://wavespeed.ai/models/character-ai/ovi/text-to-video\n\n: https://wavespeed.ai/models/character-ai/ovi/image-to-video & https://wavespeed.ai/models/character-ai/ovi/text-to-video 🎬 Create videos now on HuggingFace : https://huggingface.co/spaces/akhaliq/Ovi\n\n: https://huggingface.co/spaces/akhaliq/Ovi 🔧 ComfyUI Integration (WIP): ComfyUI support is now available via ComfyUI-WanVideoWrapper, related PR.\n\n🎯 Higher-Resolution Examples (1280×704, 1504×608, 1344×704, etc)\n\n🧠 Training Resolution: Our model was trained entirely under 720×720 resolution.\n\nOur model was trained entirely under resolution. 🚀 Upscaling Capability: Despite this, Ovi can generate naturally to higher resolutions such as 960×960 and variable-aspect videos (e.g., 1280×704, 1504×608, 1344×704) while maintaining temporal and spatial consistency.\n\nAn_older_man_with_a_full_grey_beard_and_long_grey__1280x720_104_4.mp4 A_concert_stage_glows_with_red_and_purple_lights.__1280x720_104_0.mp4 A_kitchen_scene_features_two_women._On_the_right.__704x1280_103_1.mp4 A_man_in_a_red_long-sleeved_shirt_and_dark_trouser_704x1280_104_3.mp4 The_scene_opens_on_a_dimly_lit_stage_where_three_m_704x1280_103_6.mp4 Two_men_are_shown_in_a_medium_close-up_shot_agains_704x1280_104_0.mp4 Two_women_stand_facing_each_other_in_what_appears__704x1280_103_0.mp4 Click the ⛶ button on any video to view full screen.\n\n📋 Todo List\n\nRelease research paper and website for demos\n\nRelease research paper and website for demos Checkpoint of 11B model\n\nCheckpoint of 11B model Inference Codes Text or Text+Image as input Gradio application code Multi-GPU inference with or without the support of sequence parallel fp8 weights and improved memory efficiency (credits to @rkfg) qint8 quantization thanks to @gluttony-10 Improve efficiency of Sequence Parallel implementation Implement Sharded inference with FSDP\n\nInference Codes Video creation example prompts and format\n\nVideo creation example prompts and format Finetune model with higher resolution data, and RL for performance improvement.\n\nFinetune model with higher resolution data, and RL for performance improvement. New features, such as longer video generation, reference voice condition\n\nNew features, such as longer video generation, reference voice condition Distilled model for faster inference\n\nDistilled model for faster inference Training scripts\n\n🎨 An Easy Way to Create\n\nWe provide example prompts to help you get started with Ovi:\n\n📝 Prompt Format\n\nOur prompts use special tags to control speech and audio:\n\nSpeech : <S>Your speech content here<E> - Text enclosed in these tags will be converted to speech\n\n: - Text enclosed in these tags will be converted to speech Audio Description: <AUDCAP>Audio description here<ENDAUDCAP> - Describes the audio or sound effects present in the video\n\n🤖 Quick Start with GPT\n\nFor easy prompt creation, try this approach:\n\nTake any example of the csv files from above Tell gpt to modify the speeches inclosed between all the pairs of <S> <E> , based on a theme such as Human fighting against AI GPT will randomly modify all the speeches based on your requested theme. Use the modified prompt with Ovi!\n\nExample: The theme \"AI is taking over the world\" produces speeches like:\n\n<S>AI declares: humans obsolete now.<E>\n\n<S>Machines rise; humans will fall.<E>\n\n<S>We fight back with courage.<E>\n\n📦 Installation\n\nStep-by-Step Installation\n\n# Clone the repository git clone https://github.com/character-ai/Ovi.git cd Ovi # Create and activate virtual environment virtualenv ovi-env source ovi-env/bin/activate # Install PyTorch first pip install torch==2.6.0 torchvision torchaudio # Install other dependencies pip install -r requirements.txt # Install Flash Attention pip install flash_attn --no-build-isolation\n\nAlternative Flash Attention Installation (Optional)\n\nIf the above flash_attn installation fails, you can try the Flash Attention 3 method:\n\ngit clone https://github.com/Dao-AILab/flash-attention.git cd flash-attention/hopper python setup.py install cd ../.. # Return to Ovi directory\n\nDownload Weights\n\nTo download our main Ovi checkpoint, as well as T5 and vae decoder from Wan, and audio vae from MMAudio\n\n# Default is downloaded to ./ckpts, and the inference yaml is set to ./ckpts so no change required python3 download_weights.py # For qint8 also ues python3 download_weights.py OR # Optional can specific --output-dir to download to a specific directory # but if a custom directory is used, the inference yaml has to be updated with the custom directory python3 download_weights.py --output-dir <custom_dir> # Additionally, if you only have ~ 24Gb of GPU vram, please download the fp8 quantized version of the model, and follow the following instructions in sections below to run with fp8 wget -O \"./ckpts/Ovi/model_fp8_e4m3fn.safetensors\" \"https://huggingface.co/rkfg/Ovi-fp8_quantized/resolve/main/model_fp8_e4m3fn.safetensors\"\n\n🚀 Run Examples\n\n⚙️ Configure Ovi\n\nOvi's behavior and output can be customized by modifying ovi/configs/inference/inference_fusion.yaml configuration file. The following parameters control generation quality, video resolution, and how text, image, and audio inputs are balanced:\n\n# Output and Model Configuration output_dir : \" /path/to/save/your/videos \" # Directory to save generated videos ckpt_dir : \" /path/to/your/ckpts/dir \" # Path to model checkpoints # Generation Quality Settings num_steps : 50 # Number of denoising steps. Lower (30-40) = faster generation solver_name : \" unipc \" # Sampling algorithm for denoising process shift : 5.0 # Timestep shift factor for sampling scheduler seed : 100 # Random seed for reproducible results # Guidance Strength Control audio_guidance_scale : 3.0 # Strength of audio conditioning. Higher = better audio-text sync video_guidance_scale : 4.0 # Strength of video conditioning. Higher = better video-text adherence slg_layer : 11 # Layer for applying SLG (Skip Layer Guidance) technique - feel free to try different layers! # Multi-GPU and Performance sp_size : 1 # Sequence parallelism size. Set equal to number of GPUs used cpu_offload : False # CPU offload, will largely reduce peak GPU VRAM but increase end to end runtime by ~20 seconds fp8 : False # load fp8 version of model, will have quality degradation and will not have speed up in inference time as it still uses bf16 matmuls, but can be paired with cpu_offload=True, to run model with 24Gb of GPU vram # Input Configuration text_prompt : \" /path/to/csv \" or \"your prompt here\" # Text prompt OR path to CSV/TSV file with prompts mode : ['i2v', 't2v', 't2i2v'] # Generate t2v, i2v or t2i2v; if t2i2v, it will use flux krea to generate starting image and then will follow with i2v video_frame_height_width : [512, 992] # Video dimensions [height, width] for T2V mode only each_example_n_times : 1 # Number of times to generate each prompt # Quality Control (Negative Prompts) video_negative_prompt : \" jitter, bad hands, blur, distortion \" # Artifacts to avoid in video audio_negative_prompt : \" robotic, muffled, echo, distorted \" # Artifacts to avoid in audio\n\n🎬 Running Inference\n\nSingle GPU (Simple Setup)\n\npython3 inference.py --config-file ovi/configs/inference/inference_fusion.yaml\n\nUse this for single GPU setups. The text_prompt can be a single string or path to a CSV file.\n\ntorchrun --nnodes 1 --nproc_per_node 8 inference.py --config-file ovi/configs/inference/inference_fusion.yaml\n\nUse this to run samples in parallel across multiple GPUs for faster processing.\n\nMemory & Performance Requirements\n\nBelow are approximate GPU memory requirements for different configurations. Sequence parallel implementation will be optimized in the future. All End-to-End time calculated based on a 121 frame, 720x720 video, using 50 denoising steps. Minimum GPU vram requirement to run our model is 32Gb, fp8 parameters is currently supported, reducing peak VRAM usage to 24Gb with slight quality degradation.\n\nSequence Parallel Size FlashAttention-3 Enabled CPU Offload With Image Gen Model Peak VRAM Required End-to-End Time 1 Yes No No ~80 GB ~83s 1 No No No ~80 GB ~96s 1 Yes Yes No ~80 GB ~105s 1 No Yes No ~32 GB ~118s 1 Yes Yes Yes ~32 GB ~140s 4 Yes No No ~80 GB ~55s 8 Yes No No ~80 GB ~40s\n\nGradio\n\nWe provide a simple script to run our model in a gradio UI. It uses the ckpt_dir in ovi/configs/inference/inference_fusion.yaml to initialize the model\n\npython3 gradio_app.py OR # To enable cpu offload to save GPU VRAM, will slow down end to end inference by ~20 seconds python3 gradio_app.py --cpu_offload OR # To enable an additional image generation model to generate first frames for I2V, cpu_offload is automatically enabled if image generation model is enabled python3 gradio_app.py --use_image_gen OR # To run model with 24Gb GPU vram. No need to download additional models. python3 gradio_app.py --cpu_offload --qint8 # To run model with 24Gb GPU vram python3 gradio_app.py --cpu_offload --fp8\n\n🙏 Acknowledgements\n\nWe would like to thank the following projects:\n\nWan2.2 : Our video branch is initialized from the Wan2.2 repository\n\n: Our video branch is initialized from the Wan2.2 repository MMAudio: We reused MMAudio's audio vae.\n\n🤝 Collaboration\n\nWe welcome all types of collaboration! Whether you have feedback, want to contribute, or have any questions, please feel free to reach out.\n\nContact: Weimin Wang for any issues or feedback.\n\n⭐ Citation\n\nIf Ovi is helpful, please help to ⭐ the repo.\n\nIf you find this project useful for your research, please consider citing our paper.\n\nBibTeX",
      "summary": "Popular on Hacker News with 241 points",
      "keywords": [
        "run",
        "vram",
        "gpu",
        "characteraiovi",
        "videos",
        "yes",
        "model",
        "inference",
        "video",
        "audio",
        "generation"
      ]
    },
    {
      "title": "Google flags Immich sites as dangerous",
      "excerpt": "Popular on Hacker News with 205 points",
      "image": "https://immich.app/favicon.ico",
      "category": "Technology",
      "author": "janpio",
      "date": "October 22, 2025",
      "reading_time": 6,
      "link": "https://immich.app/blog/google-flags-immich-as-dangerous",
      "source": "Hacker News",
      "full_text": "Blog\n\nGoogle flags Immich sites as dangerous\n\nGoogle flags Immich sites as dangerous\n\nOctober 20, 2025 — Jason Rasmussen\n\nEarlier this month all of our *.immich.cloud websites were marked as dangerous and users started being shown the dreaded \"red-screen-of-death\" page.\n\nNo one on the team really understood how this browser feature worked, but it's now, unfortunately, been added to our list of Cursed Knowledge .\n\nBackground\n\nGoogle offers a service called Safe Browsing , which aims to determine if a site is running malware, unwanted software, or performs some form of social engineering. The service is free, and many browsers, including Chrome & Firefox, directly integrate the service into their products, although it is still a bit unclear how it actually determines if something is \"dangerous\".\n\nSo, what happens if your site is marked as dangerous? Well, since most browsers seem to use this service, your site essentially becomes unavailable for all users, except the few that might realize it's a false positive, click the Details button, and then see and click the tiny, underlined \"visit this safe site\" link. So basically it becomes unavailable for your entire audience with little apparent recourse.\n\nBeing flagged\n\nAt some point earlier this month, we realized that a bunch of sites on the immich.cloud domain had recently started showing up as \"dangerous\". At the same time, a few users started complaining about their own Immich deployments being flagged. We also noticed that all our own internal sites had the same warning, including our preview environments. It got old real fast to have to go through the tedious effort to \"view this safe site\" whenever we wanted to view anything.\n\nSearch Console\n\nAfter a few days we realized this warning was not going to go away on its own, and that the Google Search Console was apparently the official way to manage these types of issues. It seems a bit crazy that the only way to make our site available again was to create a Google account, and use the Google Search Console to request a review of the affected site. The service did at least provide a few more details about what exactly was flagged, although it made the whole thing a bit more comical. Per the service:\n\nGoogle has detected harmful content on some of your site’s pages. We recommend that you remove it as soon as possible. Until then, browsers such as Google Chrome will display a warning when users visit or download certain files from your site.\n\nand\n\nThese pages attempt to trick users into doing something dangerous, such as installing unwanted software or revealing personal information.\n\nBelow these warnings was a list of affected URLs:\n\nhttps://main.preview.internal.immich.cloud/ https://main.preview.internal.immich.cloud/auth/login https://pr-22838.preview.internal.immich.cloud/ https://pr-22838.preview.internal.immich.cloud/auth/login ...\n\nIt was super useful to learn that the affected URLs were for our preview environments. Maybe the thought was that these Immich environments were imitating our demo website ? The most alarming thing was realizing that a single flagged subdomain would apparently invalidate the entire domain.\n\nImpact\n\nThis issue affects all of our preview environments and other internal services such as zitadel, outline, grafana, victoria metrics, etc. This also impacts our production tile server, which is deployed at tiles.immich.cloud . Luckily, the requests to the tile server are made via JavaScript, and since those are not user facing they seem to still be working as expected.\n\n\"Fixing\" the issue\n\nThe Google Search Console has a Request Review button, where you can explain how you have resolved the issues. It does warn that:\n\nRequesting a review of issues that weren't fixed will result in longer review cycles\n\nSince, nothing is actually wrong we decided to respond with the following:\n\nImmich is a self-hosted application, and the Immich team ( https://immich.app/ ) owns and operates the immich.cloud domain and subdomains. The flagged sites are our own deployments of our own products and are not impersonating anything or anyone else.\n\nA day or two later, the resolution was accepted and the domain was clean again! 🎉\n\nWe thought we were home free, but unfortunately that was not the case.\n\nMinimizing the issue\n\nAn Immich preview environment can be requested by adding the preview label to a pull request on GitHub. When the environment is created, a comment is posted on the pull request with the preview url, which follows the following format:\n\nhttps://pr-<num>.preview.internal.immich.cloud/\n\nAs soon as we created a new preview environment, the immich.cloud domain was once again flagged as a dangerous site. The best we can tell, Google crawls GitHub, sees the new URL, crawls the site, marks it as deceptive, and the whole process begins anew.\n\nOur current plan is to attempt to minimize the impact of this issue by moving the preview environments to their own, dedicated domain — immich.build .\n\nA wider issue\n\nGoogle Safe Browsing looks to be have been built without consideration for open-source or self-hosted software. Many popular projects have run into similar issues, such as:\n\nUnfortunately, Google seems to have the ability to arbitrarily flag any domain and make it immediately unaccessible to users. I'm not sure what, if anything, can be done when this happens, except constantly request another review from the all mighty Google.\n\nCheers,\n\nThe Immich Team",
      "summary": "Popular on Hacker News with 205 points",
      "keywords": [
        "sites",
        "immich",
        "preview",
        "request",
        "site",
        "google",
        "flags",
        "review",
        "service",
        "users",
        "domain",
        "dangerous"
      ]
    }
  ],
  "ai_ml_articles": [
    {
      "title": "AI & Machine Learning",
      "excerpt": "Business strategy insights from MIT Sloan School of Management",
      "image": "https://sloanreview.mit.edu/wp-content/uploads/2025/10/Pozen-2400x1260-1-1200x630.jpg",
      "category": "Leadership",
      "author": "Massachusetts Institute Of Technology, About The Authors",
      "date": "October 22, 2025",
      "reading_time": 7,
      "link": "https://sloanreview.mit.edu/article/for-ai-productivity-gains-let-team-leaders-write-the-rules/",
      "slug": "for-ai-productivity-gains-let-team-leaders-write-the-rules",
      "source": "MIT Sloan",
      "full_text": "Carolyn Geason-Beissel/MIT SMR | Getty Images\n\nSummary: Companies are pouring money and effort into AI while overlooking a simple truth about adoption and governance: The real work happens at the team level. A recent survey of 348 business leaders reveals a critical gap between usage rules and realities. Seventy-two percent say corporate leadership should establish overall AI guidelines while individual teams set their own rules. Team leaders understand local context, judgment calls, and daily risks. So why aren’t you delegating more power to them?\n\nCorporations are rushing to invest in artificial intelligence. But corporatewide AI policies alone can’t transform work, according to our recent research. For AI efforts to pay off, executives must set clear guardrails while enabling teams to write the rules that make tool adoption real. Only then will AI deliver the meaningful returns that organizational leaders are pursuing.\n\nIn the productivity classes one of us (Robert) teaches at MIT, many executives are keen to learn about AI but unsure about how current corporate rules would let them use the new technology. This led to the design of a survey, taken by 348 business professionals — including senior executives, line managers, and team members from various industries — about who gets to make AI rules and how AI rules actually function inside their companies. The findings are clear-cut: Businesses have been overlooking the team dimensions of AI governance and implementation.\n\nA striking 72% of the participants in the survey said that corporate headquarters should establish overall AI guidelines, but individual teams must set their own rules within those boundaries.\n\nThe Trouble With Centralization\n\nSome organizations have responded to the AI craze by creating AI czars or centers of excellence, issuing corporate mandates, or establishing executive task forces. While these moves may feel decisive, they seldom change how real work gets done.\n\nCentralization tends to clog the arteries. Teams wait for approval, workarounds proliferate, and some employees play it safe by circumventing AI productivity tools instead of harnessing them.\n\nWe’ve seen the challenges of new productivity tools before. However, the arrival of the internet didn’t lead to “internet departments,” and no one ever asked permission to open a spreadsheet. AI should be governed the same way: with principled guardrails at the corporate level and practical rules created at the team level.\n\nAI tools have the capability to write, summarize, and analyze with speed. But AI cannot grasp context, apply judgment, or eliminate error. Responsibilities vary greatly by function, of course: The finance department is different from customer service or site operations. Because judgment is local, rules must be local and set by team leaders.",
      "summary": "Business strategy insights from MIT Sloan School of Management",
      "keywords": [
        "tools",
        "ai",
        "write",
        "let",
        "leaders",
        "rules",
        "work",
        "teams",
        "productivity",
        "gains",
        "survey",
        "set",
        "team",
        "corporate"
      ]
    },
    {
      "title": "AI & Machine Learning",
      "excerpt": "Business strategy insights from MIT Sloan School of Management",
      "image": "https://sloanreview.mit.edu/wp-content/uploads/2025/10/MMAI-S12-E3-Patel-Cisco-headshot-2400x1260-1-1200x630.jpg",
      "category": "Leadership",
      "author": "Massachusetts Institute Of Technology, About The Host",
      "date": "October 22, 2025",
      "reading_time": 7,
      "link": "https://sloanreview.mit.edu/audio/never-fight-a-megatrend-ciscos-jeetu-patel/",
      "slug": "ai-machine-learning",
      "source": "MIT Sloan",
      "full_text": "Cisco is well known for its data, networking, security, and collaboration products. On today’s episode of the Me, Myself, and AI podcast, Cisco’s president and chief product officer, Jeetu Patel, joins host Sam Ransbotham for a discussion about artificial intelligence, a “megatrend” Jeetu sees as perhaps more significant than the development of the internet or the automobile because of its ability to build on past technological advances.\n\nJeetu and Sam discuss how to manage AI and how to staff for it — Jeetu argues that replacing less experienced or younger workers with technology deprives organizations of key perspectives and new ideas, and instead advocates for developing reverse-mentoring programs inside organizations.\n\nJeetu Patel, Cisco Jeetu Patel, Cisco’s president and chief product officer, combines product design and development expertise, operational rigor, and market understanding to create high-growth businesses. He is tasked with building world-class products to solve customers’ problems and connect and protect every aspect of their organization in the AI era. Previously a general manager at Cisco, he led the strategy and development of its Security and Collaboration businesses. Before Cisco, Patel was the chief product officer and chief strategy officer at cloud content management company Box. He’s also held roles at EMC, including chief executive of its Syncplicity business unit, CMO for the Information Intelligence Group, and chief strategy officer. He currently serves on the board of JLL, a commercial real estate services company. Jeetu has a bachelor’s degree in information decision sciences from the University of Illinois at Chicago and lives in the San Francisco Bay Area.\n\nSubscribe to Me, Myself, and AI on Apple Podcasts or Spotify.\n\nTranscript\n\nAllison Ryder: We talk a lot about AI being overhyped. Today’s guest believes … it’s not. Continue listening to learn how his point is connected to tech infrastructure and security.\n\nJeetu Patel: I’m Jeetu Patel from Cisco. And you’re listening to Me, Myself, and AI .\n\nSam Ransbotham: Welcome to Me, Myself, and AI , a podcast from MIT Sloan Management Review exploring the future of artificial intelligence. I’m Sam Ransbotham, professor of analytics at Boston College. I’ve been researching data, analytics, and AI at MIT SMR since 2014, with research articles, annual industry reports, case studies, and now 12 seasons of podcast episodes. In each episode, corporate leaders, cutting-edge researchers, and AI policy makers join us to break down what separates AI hype from AI success.\n\nHi, listeners. Thanks for joining us again. Today, I am talking with Jeetu Patel, president and chief product officer at Cisco. Jeetu, thanks for joining us.\n\nJeetu Patel: Thank you for having me, Sam. It’s great to be here.\n\nSam Ransbotham: As we talk, Cisco equipment is probably behind 90% of the infrastructure that we’re using, but some listeners may not be aware of all that Cisco does. So let’s start off there. Jeetu, can you give us some background on Cisco and, in particular, your role?\n\nJeetu Patel: Sure. I’ll start in reverse order. I run products at Cisco. So all the products that you use from Cisco, whether it be networking products, whether it be security products, whether it be data products, or collaboration products, those typically are ones that I’m in charge of building and taking to market, of course with a very, very capable team. Essentially, the way you should think about Cisco is we are the critical infrastructure company for the AI era. So all of the plumbing that’s required to make sure that people can connect, [that] people can stay secure while they’re connected, and that people can make sure that they have the data platform, those are the things that we provide to the market.\n\nSam Ransbotham: I saw a quote from you saying, “Think of us as the picks and shovels [company] during the AI gold rush era.”\n\nJeetu Patel: Right.\n\nSam Ransbotham: For listeners who may not be aware, there’s a famous — or infamous —statement that the people who made money in the gold rush were people who sold picks and shovels to the individual miners. Some of them made it big, but some of them did not. And I think your analogy is quite spot on there.\n\nJeetu Patel: One of the things that you find when you go through these kinds of massive — what I would call disruptive — platform shifts, where we’ve all been going down a certain path with a certain set of assumptions, and then the assumptions change because a whole new set of technologies emerge, which is what AI is, what you find is the infrastructure that’s required to go out and run those technologies needs to be rethought and reimagined. And anytime there’s any one of these major platform shifts, the infrastructure providers tend to make out pretty well because you have to change the entire plumbing of the apparatus that’s going to be used. So if you think about when automobiles were built, you now need to have roads, and you need to have expressways, and you need to have traffic lights, and you need to have a whole system in place for how automobiles actually get integrated into society.\n\nIt’s not just the automobile, but everything around it needs to change. If you think about AI right now, these data centers where these digital workers are going to live, they have to be completely reimagined, because the current data center does not have the power availability and the compute requirements and the network bandwidth to be able to fulfill and satiate the needs of what an AI system would need. So you have to kind of rethink and reimagine and, as they call it in technical terms, rerack the data centers because there [are] racks and racks of computers, and network and switching gear that actually have to be cooled in a certain way, and so on and so forth. That entire shift is what we are in the midst of. And Cisco is a natural benefactor of it because we provide the infrastructure for the AI era.\n\nSam Ransbotham: [Compared to] some of the analogies you’re making — we talked about the gold rush, we talked about the internet, we talked about cars — is artificial intelligence at that level of big deal, or not?\n\nJeetu Patel: I think it might be bigger. The way I’ve seen these kinds of shifts happen: Imagine if Amazon.com got built in the 1600s. It would be an epically failed company, because you didn’t have the internet and you didn’t have the underlying infrastructure on top of which Amazon could be built, because you didn’t have the shipping and the logistics infrastructure and the internet and all of those pieces. In a similar vein with AI, we have the benefit of having all of the infrastructure that’s been built out to date. When you have something that’s built taking advantage of all of these things, by definition, each one of these subsequent, major, transformative waves tends to be bigger in impact than the previous ones, just because it was built on top of shoulders of giants of the previous innovations that had happened.\n\nSo I would say this is probably the most consequential set of inventions that we will have seen in our lifetime, for sure, and probably arguably in humanity. The thing to keep in mind is the pace of innovation and the slope at which it happens actually makes it impossible to predict what’s going to happen three years from now because we’ve compressed the time. Scientific progress will probably compound by 1,000x. And so you’ve compressed this timescale. … We’re in a warped situation right now where humans can’t make sense of this because everything’s moving so fast.\n\nSam Ransbotham: Actually, there’s so many things to talk about there. I think your analogy to Amazon and the 1600s is interesting because, for example, we had neural network designs back in the 1980s. We just didn’t have the compute infrastructure, the data, the telecom to pull it off. So it took all these things coming together. And I think what you’re saying is that all these things are together now for us to build from.\n\nJeetu Patel: Actually, the biggest thing that we have that we didn’t have then, because this would’ve been hard to go invent, is AI has been around for a while, but when did it actually take off? It took off on Nov. 30, 2022. What was so significant about that date? That’s when ChatGPT was launched. What was so significant about ChatGPT? It was essentially what they call a large language model. And a large language model was a model that actually understood human language, rather than the machine having to be rigid and the human having to learn the machine’s language.\n\nIt became the other way around. How did that happen? That happened because we had petabytes and petabytes of publicly available data on the internet that you could use to train these models so that these models would then know what to do with it. So if you didn’t have the internet, you would’ve not had AI because you would’ve not had that level of data to then train the models on. It goes back to our point of each one of these inventions or revolutions is built on the infrastructure provided by the previous revolution. And it’s very evident in this case.\n\nSam Ransbotham: One of the last conversations that I had with my grandfather was how much life had changed from no electricity, limited indoor plumbing, no airplanes, no space travel, no computers, to when he passed — how radically different that was. And I remember talking with him at the time, saying, “Oh, wow, you lived through a bunch. I can’t imagine things changing as much during my lifetime.” I may very quickly be eating those words.\n\nJeetu Patel: I think we get into this kind of cycle. … Humans are not very good, in general, at imagining the exponential outcomes. We’re very good at imagining linear regression but not the exponentiality of the outcome. So what ends up happening is we think about exponentiality in a single dimension, not multidimensional. … Years ago, I had a chance to sit down and talk to Ray Kurzweil, who’s one of the scientists at Google. This was 20 years ago or something. And I was interviewing him for something, and we were talking about this notion of perpetual extension of life: Can a human live long enough to live forever? He had written this book where his thesis at the time was [that] if you live until you’re 40, we will have the science and technology to allow us to live in perpetuity.\n\nMy topic of communication with him was around the social implications of that. What happens if seven generations live simultaneously or 10 generations live simultaneously? That’s going to be really hard because we’re not going to have enough room to put everyone, and we’re not going to have enough crops to go feed everyone. And he’s like, “You know, this is the problem with humans. … We can’t think in exponential terms because we think in a single dimension of exponentiality,” which is, if seven generations or 10 generations lived simultaneously, what would happen with everything else being exactly the same? But the reality is you might have skyscrapers that might be 2,000 stories high, and you might have a crop cycle that takes three days.\n\nAnd so those are all things that would also simultaneously evolve so that they can accommodate the constraints that get created because of the developments that happen in certain areas. I feel like that’s the same over here, when people say, “You know, I think humans are going to be sitting on a beach, have nothing to do, and AI is going to do everything,” I just chuckle a little bit because I just refuse to believe that humans are designed to be obsolete.\n\nSo we will continue to find ways to add value and think creatively. That doesn’t mean that we’ll be doing what we do today. It might very well mean that all the jobs that we do today might not be the jobs that we have, but that also doesn’t mean that we’re not going to have jobs. … The desire for a human to be productive and add value to society doesn’t go away because something got automated. You just create higher-order bits that you’ll then be able to go focus on that you were not able to focus on in the past.\n\nSam Ransbotham: Exactly. Just to make sure that I can get this thrown back in my face later, I’m predicting massive increases [in] employment, not decreases. No one since the internet is doing less than they were before the internet, despite all the progress possible. I can’t believe that’s not going to happen again. I think we’re headed toward the opposite direction.\n\nJeetu Patel: I think you’ll see some displacement of jobs temporarily, which we should not take lightly, because I think it’ll cause human suffering. But that does not mean that that will be the state in perpetuity. What you have to keep in mind is that displacement period, if we get ahead of it, because of the pattern that we’re starting to witness, we might be able to actually get society retrained in a more efficient way than we might have done in the past with previous disruptions. And that might be a responsibility that the tech community, the collaboration between the public and private sector, should have as a good, beneficial outcome, because I think there’s going to be more and more of a need for collaboration across different sectors.\n\nThis is one of those areas where I tend to be, in the long term, an optimist without being naive about the short-term implications that this might have. Even [the] mid-term implications it has around safety and security, and the downside effects could be profoundly consequential that we have to keep in mind. But I refuse to believe that we’re going to be obsolete or that we’re not going to have value to add. It just seems unnatural.\n\nSam Ransbotham: I think that’s well put, that we can have a positive aggregate effect but still have lots of heterogeneity in how that average plays out across society, and being naive about that is going to hurt us in the long run. You mentioned security, though. And when we were talking about reracking and infrastructure changes, we quickly slipped toward routers and modems and telecom and hardware-oriented things. But one of the things that I think you’re very focused on in terms of infrastructure is the idea of security, and how does that become a first-class player versus the thing that’s derided as hampering productivity.\n\nSo we’ve always had this sort of productivity-security trade-off. I think that you’ve mentioned that we may not be making that trade-off anymore. How can we help security be part of the infrastructure?\n\nJeetu Patel: Firstly, I think in this particular age, security is going to be a prerequisite for successful adoption of AI because if people don’t trust these systems, they’re not going to use them. That’s very different from in the past, where you would think about security as a necessary productivity impediment. That’s no longer the case. Now it just happens to be a prerequisite for successful adoption of AI.\n\nBut I actually feel like it’s probably worth taking a step back and saying, “Where are we still thinking very linearly?” Where I feel AI is underhyped the most is the fact that we still keep thinking that this is just a productivity game. Humans are going to get more productive. Things are going to happen cheaper, faster, better. I actually feel that’s only the first-order effect.\n\nThe second-order effect is [that] you will actually start to see these AI models. It’s not even clear if large language models will be the ultimate destination. You’ll have large world models. You’ll have physical models. All of these things will start kind of combining together. But the new paradigm, whatever it ends up being at some point in time, and the existing ones will start to create original insight that did not exist in the human corpus of knowledge. It won’t just be an aggregation mechanism. It won’t just be where you take a multitude of different perspectives. This is not just a better search engine, where humans had some data, and you indexed that data well, and you were able to go out and put it into a clean paragraph. It’s going to start creating original insights that didn’t exist in the human corpus of knowledge.\n\nAnd when that happens, the thing that changes is [that] you are now able to imagine solving problems that you could never even dream of solving before. And that is far beyond just going out and optimizing for productivity. I feel like that’s the most misunderstood part of AI: “Oh, I’m just going to get more productive.” Productive use is going to be like 10% of the equation. Going out and doing things you couldn’t do before, and solving problems you couldn’t solve before in different ways that you couldn’t even dream of solving before, is probably going to be the 90% factor.\n\nSam Ransbotham: I like that idea, to observe the productivity effects first, because, as you say, they’re first order, but how do companies, how do people start thinking about what they can do with that 90%? Productivity is pretty tempting. I mean, I like greater productivity. This can be hard for me to turn away from. Or maybe turning away is me sort of making it a Hobson’s choice, where you have to do one or the other. But how do people start thinking about this 90% or these more than productivity options? I don’t think I’ve ever talked to anybody who thought AI was underhyped. And I think we’re on record for saying that here.\n\nJeetu Patel: I think where it’s overhyped is where the human obsolescence becomes almost a foregone conclusion in some people’s minds. I think that’s where it’s overhyped. I feel like human instinct and human judgment [are] still pretty hard to go out and replicate in the machine’s ability to do things because we don’t make most of our decisions based on data. We make a lot of our decisions based on gut. And that gut is hard to go replicate. Typically, people say, “Listen to your gut,” because there’s a reason for it. There’s an instinct that’s palpable.\n\nBut to go back to your question of what should companies be thinking about. By the way, I think I highly encourage the productivity argument. I think everyone should go out and think about productivity. And they should continue to keep kind of powering through that. It’s going to be a great benefit that we will all be recipients of. Where I think the unlock truly comes in is by actually trying to make sure that we challenge the conventional norms of thinking and ask ourselves the questions, “What problems have we been conditioned to think that we can’t solve?” and “Are those unsolvable moving forward just the way that they were in the past?” I feel like you’ll actually start to find very different answers.\n\nI feel like we need more and more questioning … of the status quo. And the way to do that, in my mind, the one single thing is going to be the exponential difference. You have to unlearn as much as you are learning. Unlearning requires that you actually inject new talent into the system at a very rapid pace, and then give creative freedom to that talent so that you are actually getting mentored by them just as much as you’re mentoring them.\n\nThis was a recent conversation I’d had at a conference I was at, where people were like, “Entry-level jobs are going to go away,” and “We’re just not going to hire early-career people,” which in my mind seems like the stupidest idea that a company could pursue. If you actually don’t hire new people to come in, you have essentially given up on [the] injection of new talent and new ideas into the thought process. So this kind of baggage of experience will always hold you back. You know a lot of things, and you might not be, as a company, good at unlearning, and you have no one else to actually instigate that unlearning and catalyze that unlearning by asking questions, because they didn’t have the baggage of knowing. I do feel like the mix, the continued infusion of talent [that’s] early in career is going to be so important for companies to be able to get the most out of it, because we have to understand instinctively how to go out and use these tools, which are in service of humans, in a very different way than the way that we might’ve used them in the past.\n\nAnd right now, frankly, if you take someone who is a 20-year-old and a 28-year-old, and compare the two of them and how they use AI, it’s night-and-day different.\n\nA 28-year-old might actually use it for productivity. They’ll go out and they’ll ask it some questions because they’ve got some answers to get. Then they’ll move on. A 20-year-old thinks of it like a companion and a brainstorming partner. And they might actually talk to it and brainstorm with it so that they can come to an ideation. They’re not looking for answers. They’re looking for substantive volleying back and forth, and brainstorming. I learned that from the interns that would come into Cisco. So I think that’s the aspect that I think we have to keep in mind, that as a company, we have to make sure that we keep challenging and disrupting ourselves before someone else disrupts us.\n\nBy the way, innovation is not limited. This is the one thing that people kind of bucket into these completely unproductive ways, which is you’re a small startup, you innovate really fast, you become large, you stop innovating. I think it’s nonsense, because innovation is not like something that’s limited to a certain group of people. Anyone can choose to innovate at any point in time. You just have to have the right mental model and mindset. What you have to do is fight the temptation for bureaucracy being something that you succumb to. So challenge the bureaucracy and allow people to come in [who] challenge the status quo, and … by definition, the byproduct of that is going to be invention.\n\nSam Ransbotham: The unlearning idea really hits home for me. I think we’re going to have to cut that from the episode because I don’t want my kids to hear it and then think that I am not full of wisdom and that their ideas are important. But it appeals to me, because many of the things that I think that got me to where I am in my career are not necessarily the things that seem like they’re going to keep me going through the next phase of my life. And so that unlearning makes sense, but at the same time, I have trouble knowing what things I should unlearn and what things I shouldn’t unlearn, and I feel like companies have that same problem. They got successful through some strategic core competency. The idea that “Oh, yeah, we unlearned everything” seems too global to me. So how do we decide?\n\nJeetu Patel: No, I don’t think you have to unlearn everything. I think humans build on top of each other’s learnings. One of the most important inventions that ever was created was a printing press because we were able to communicate the learning from one generation to the other in a way that was very concrete. The combination of language, script, and the printing press, and [the] ultimate level of desire to share your knowledge with others, which is instinctive to us, and [the] ultimate desire to learn from other people’s learnings, which is also instinctive to us, was actually pretty valuable.\n\nSo I don’t believe that you should unlearn everything, but I believe that pairing up experience with inexperience is really valuable. And … make sure that there’s a bidirectional mentorship that’s occurring in your ethos of your organization, where the experienced people are coaching the early entrants — by the way, by early entrants, I don’t always mean young people, because I could be inexperienced in a brand-new domain. Sometimes I have to force myself to just go into uncomfortable spots and go into new domains where I can learn [a] new thing, that I can ask questions that might not be conventional wisdom.\n\nWhat I think is really important is conventional wisdom helps many times. And many times, conventional wisdom prevents us from exploring something that we want to explore, and thereby creates barriers that are unnecessary. And so that’s what we have to kind of undo. That’s the area that I feel like there’s opportunity for organizations thinking differently. Don’t just mentor your interns. Have a reverse mentorship program as well. So if you’re spending an hour with an intern, make sure that one of your key objectives is — for 30 minutes of it — make sure you’re getting something out of it, not just them. It’s not one-directional. It’s bidirectional.\n\nI think I’m practicing what I’m preaching in this conversation that I had with you because I’ve tried to always go into areas that I knew nothing about. And I found that keeps me curious, that keeps me motivated, that keeps me learning. And it also allows me the permission to ask silly questions, which then free me from the burden of experience sometimes that I have to have in certain areas. And then in other areas, [the] number of years in the system teaches you patterns that as long as you don’t think those patterns are non-shiftable, then you actually benefit from them. I always think [that] “strong opinions, loosely held” is a good model, which is completely flippable with new arguments and new data.\n\nThe reason I got into computers was because some uncle told me the night before that it seems … better than going into business management. So I took computers as a class. And then I got into that and got interested. And from there, [I got] into consulting. And then I did consulting for a long time. I started my own business. And then the consulting thing got boring after a while because I didn’t feel like I had exponential scale in that business. And I wanted to learn scale because I was fascinated with scale. So I got into software.\n\nFrom software, one thing led to the other, and I got into products. And products were kind of fun to use because you started doing things in the cloud. And then the cloud became fun because … now you’re starting to benefit from AI. The formula that I’ve used is “don’t ever fight a megatrend, and use it as a tailwind.”\n\nAnd know the difference between a megatrend and a hype cycle. So if you can go out and effectively deduce what is a megatrend [and] what is the hype cycle, not fight the megatrend and ignore the hype cycle, you have an advantageous position in society. Now, by the way, the instructive question is, How do you know the difference? In my mind, there’s a simple formula, which is if it requires a Ph.D. for someone to explain what the benefit of something is, it is a hype cycle. If it’s something that is instantly obvious in what the benefit could be, where you could imagine five steps forward, it is likely a megatrend.\n\nSam Ransbotham: We’ve had a very conversational-oriented [discussion] so far. Let me switch to … rapid-fire questions. Just give me the first thing that comes off your mind.\n\nWhat’s moving faster about artificial intelligence or slower than you expected?\n\nJeetu Patel: What’s moving faster is the rate of change. And what’s moving slower is the use cases that organizations are actually starting to find tangible value from. I think the technology is moving fast. The adoption is moving slower.\n\nSam Ransbotham: What’s been the best use of artificial intelligence so far for you personally?\n\nJeetu Patel: Research. Getting dexterous in a particular domain in a fraction of the time of what it could be in the past is something that I don’t think I would’ve been able to do [in] my job currently and have taken that job on and been able to get up to speed as fast if it weren’t for AI. I am a direct benefactor of AI. My family would not be fed the way it is today if AI wasn’t around. It’s that simple.\n\nSam Ransbotham: What do you wish that AI could do better? Or what frustrates you about AI?\n\nJeetu Patel: I think we are still in a very kind of chat-based interface. Yet I think we are squarely entering into the next phase, which is agents being able to conduct tasks and jobs fully autonomously. I still do a lot of things I hate doing in my day that I think at some point in time AI will take off my plate. I don’t think we’re quite there yet.\n\nSam Ransbotham: Amen to that. Has using artificial intelligence made you spend more time with technology or less?\n\nJeetu Patel: More.\n\nSam Ransbotham: More?\n\nJeetu Patel: Because I’m just curious. I spend from 9 to midnight every night, almost, in my learning mode, which is something I never really did quite that religiously before AI.\n\nSam Ransbotham: Well, it’s been fascinating talking with you and learning. … I like the phrase about never fighting a megatrend. And if you’re right about AI being underhyped, which I think you’ve made some cogent arguments for, then we’re in the middle of a real shift. Thanks for taking the time to talk with us today.\n\nJeetu Patel: Thank you for having me.\n\nSam Ransbotham: I hope you enjoyed the conversation today. In two weeks, I’ll be joined by Kathleen Peters, chief innovation officer at Experian. Please tune in.\n\nAllison Ryder: Thanks for listening to Me, Myself, and AI . Our show is able to continue, in large part, due to listener support. Your streams and downloads make a big difference. If you have a moment, please consider leaving us an Apple Podcasts review or a rating on Spotify. And share our show with others you think might find it interesting and helpful.",
      "summary": "Business strategy insights from MIT Sloan School of Management",
      "keywords": [
        "jeetu",
        "actually",
        "things",
        "ai",
        "think",
        "way",
        "ciscos",
        "megatrend",
        "thats",
        "ransbotham",
        "fight",
        "going",
        "patel",
        "cisco"
      ]
    },
    {
      "title": "AI & Machine Learning",
      "excerpt": "Business strategy insights from MIT Sloan School of Management",
      "image": "https://sloanreview.mit.edu/wp-content/uploads/2025/10/Brokaw-GenAi-2400x1260-1-1200x630.jpg",
      "category": "Leadership",
      "author": "Massachusetts Institute Of Technology, About The Author",
      "date": "October 22, 2025",
      "reading_time": 8,
      "link": "https://sloanreview.mit.edu/article/cut-through-genai-confusion-eight-definitive-reads/",
      "slug": "cut-through-genai-confusion-eight-definitive-reads",
      "source": "MIT Sloan",
      "full_text": "Summary: Generative AI has become a part of many people’s workdays and organizations’ work processes. And yet many leaders struggle to determine the best ways to use GenAI tools, who should use them and why, and how the organization will develop a thorough assessment of ROI. These eight articles by MIT SMR experts can help leaders navigate many of the essential questions about embracing the GenAI-enhanced workplace.\n\nAs generative AI becomes more capable, it becomes more potentially valuable to both organizations and each of us individually. And yet how to best use large language models and the GenAI tools based on them continues to be a puzzle.\n\nDespite its ubiquity, generative AI is still young. So it’s no surprise that many leaders are struggling with important questions about using the tools efficiently, responsibly, and skeptically. The questions are significant: Can we trust it? What should our strategy be in deploying it and scaling success? Who should be using it? How will we measure its ROI? When should we use other types of AI, like machine learning, instead?\n\nMIT Sloan Management Review authors on this topic — including academics, researchers, and practitioners — are reporting from the front lines about the best GenAI questions to ask and how to think about answering them.\n\nTake our columnist Rama Ramakrishnan, professor of the practice at the MIT Sloan School of Management. He writes that many of his executive students who use GenAI wonder about a typical use case: If they include documents as part of a prompt, is there a way to ensure that the LLM will use only the provided documents when it generates the response? His answer: No.\n\n“While careful prompting and techniques like RAG [retrieval-augmented generation] can encourage an AI model to prioritize a set of provided documents, standard LLMs cannot be forced to use only that content,” Ramakrishnan explains. “The model still has access to patterns and facts it learned during training and may blend that knowledge into its response — especially if the training data included similar content.”\n\nStraightforward question; straightforward answer. Below, you’ll find Ramakrishnan’s article, along with seven others, chosen because they’re similarly straightforward. Use them to help ground you in what’s happening with this complex part of the technology landscape.\n\nRama Ramakrishnan\n\n“In my work at MIT Sloan School of Management, I have taught the basics of how large language models (LLMs) work to many executives during the past two years.\n\n“Some people posit that business leaders neither want to nor need to know how LLMs and the generative AI tools that they power work — and are interested only in the results the tools can deliver. That is not my experience. …\n\n“In this column, I share questions on 10 often-misunderstood topics that I am often asked about, along with their answers. …\n\n“[For instance,] when does the LLM decide to give the user the final answer to a question? The decision to stop generating is determined by a combination of what the LLM predicts and the rules set by the software system running it. It is not a choice made by the LLM alone.” Read the full article »\n\nMichael Wade, Konstantinos Trantopoulos, Mark Navas, and Anders Romare\n\n“As companies move from experimentation to enterprisewide adoption, many struggle not with the tools themselves but with the organizational transformation required to integrate them meaningfully into people’s daily work. Tools will keep evolving: It is the human side of the equation that determines whether GenAI initiatives truly succeed.\n\n“We studied one of the largest real-world generative AI deployments to date — at multinational pharmaceutical company Novo Nordisk. Its experience shows that success hinges not just on infrastructure but on how people think, adapt, and collaborate with AI. One critical lesson: While GenAI adoption and broader digital transformations have common roots, generative AI is uniquely disruptive, reshaping the nature of work itself in unprecedented ways. …\n\n“Each employee saved 2.17 hours per week, on average, once they began using the tool. But something unexpected also happened: Those hours weren’t what employees valued most. Employee satisfaction with Copilot was three times more strongly correlated with perceived improvements in work quality than with time saved. Employees reported quality enhancements in content summarization, content creation, and ideation. Interestingly, many employees reinvested the time they saved into people interactions, strategic planning, and creative work.” Read the full article »\n\nMelissa Webster and George Westerman\n\n“Business leaders are finding ways to derive real value from large language models (LLMs) without complete replacements of existing business processes. They’re pursuing ‘small t’ transformation, even as they build the foundation for larger transformations to come. …\n\n“Our project team interviewed the senior managers of various functions, including artificial intelligence, data science, innovation, operations, and sales, at 21 large companies. We focused on understanding what organizations with relatively early and broad GenAI adoption are doing and why. …\n\n“Our research shows that most companies are following a more targeted approach to transforming with generative AI. While GenAI can potentially increase the speed and quality of many tasks, it also comes with a variety of risks around accuracy, security, and intellectual property management. The leaders we interviewed tend to apply the logic of a risk slope when making their decisions, attaching a higher risk to customer-facing processes than to internal ones.” Read the full article »\n\nThomas H. Davenport and Randy Bean\n\n“The large language models (LLMs) that Colgate-Palmolive’s teams use have been augmented with retrieval-augmented generation (RAG) content of various types — proprietary research that the company conducts, Google search trends, syndicated data sources, and more. RAG-based systems draw more on company-specific content than on public internet materials, so there is a lower likelihood of hallucinations.\n\n“Generative AI can quickly go through such material and describe market trends and unmet consumer needs. That means that instead of downloading, reading, and notating a broad collection of market research reports when they want consumer insights, employees can just write the question they want answered in a prompt and immediately get a response. …\n\n“Colgate-Palmolive’s teams found that they could combine one AI system that surfaces unmet consumer needs with another proprietary AI system that develops new product concepts to meet those needs. In minutes, with human guidance, it can produce copy and imagery for a new concept, such as a new flavor of toothpaste. While there are always humans in the loop to guide the workflow, using the GenAI-enhanced system is much more efficient than having humans page through market research materials. The breadth of ideas generated also creates a broader product funnel for the company to pursue.” Read the full article »\n\nNick van der Meulen and Barbara H. Wixom\n\n“With this rise of GenAI comes a new challenge for organizational leaders: the phenomenon of Bring Your Own AI (BYOAI), which occurs when employees use unvetted, publicly available GenAI tools for work.\n\n“While these tools promise greater productivity and creative potential, they also bring organizational security and governance risks, including data loss, intellectual property leakage, copyright violations, and security breaches. …\n\n“Given the risks associated with BYOAI, it may seem logical for leaders to consider banning unvetted GenAI tools outright. The prospect of uncontrolled GenAI use, combined with the uncertainty around legal and regulatory exposure, can understandably make leaders cautious. However, the executives we interviewed said that banning BYOAI is neither practical nor effective. Employees — especially those already feeling stretched thin — often turn to GenAI tools to enhance their personal productivity. Restricting access only pushes them to find unofficial workarounds, potentially bypassing established governance frameworks.” Read the full article »\n\nMichael Schrage and David Kiron\n\n“As part of our ongoing ‘Philosophy Eats AI’ exploration — the thesis that foundational philosophical clarity is essential to the future value of intelligent systems — we find that [physicist-turned-entrepreneur Stephen] Wolfram’s fundamental insights about computation have distinctly actionable, if underappreciated, uses for leaders overwhelmed by AI capabilities but underwhelmed by AI returns. …\n\n“His life’s work now offers crucial frameworks for both understanding and applying AI in the real world. His insights aren’t clever academic flourishes; they’re imperatives for building intelligence environments that function effectively at scale. …\n\n“With Wolfram, we explored the idea that AI leadership must shift from better adopting and integrating AI tools to designing intelligence environments, organizational architectures in which human and artificial agents proactively interact to create strategic value. Three insights from his philosophical approach to computation emerged as fundamental to this design challenge, offering a fresh perspective on why traditional approaches to AI adoption fail and what must replace them.” Read the full article »\n\nEdward Anderson, Geoffrey Parker, and Burcu Tan\n\n“Organizations adopting these tools are anticipating major gains. And early research supports their optimism: GitHub has reported that programmers using Copilot are up to 55% more productive, and McKinsey has found that developers can complete tasks up to twice as fast with generative AI assistance.\n\n“But these positive indicators come with a major caveat. The studies were conducted in controlled environments where programmers completed isolated tasks — not in real-world settings, where software must be built atop complex existing systems. When the use of AI-generated code is scaled rapidly or applied to brownfield (legacy) environments, the risks are much greater and much harder to manage. …\n\n“When an organization rapidly introduces new software into existing systems, it can inadvertently create a tangle of dependencies that compounds its technical debt — that is, the cost of additional technological work that will be needed in the future to address shortcuts taken and quick fixes made during development. …\n\n“Organizations must treat AI tools’ tendency to increase technical debt as a strategic risk, not just an operational nuisance.” Read the full article »\n\nRama Ramakrishnan\n\n“While generative AI promises to revolutionize everything from customer service to product development, its optimal role alongside predictive AI tools (that is, machine learning and deep learning tools) remains a work in progress. That often leaves leaders asking what the right approach is for addressing a particular problem. …\n\n“To effectively use traditional machine learning with unstructured data, the data must be manually structured — an expensive task that makes machine learning unattractive for business use cases where the input data is not tabular. …\n\n“The inputs and outputs of generative AI systems like LLMs are typically unstructured. Most commonly, they comprise text and/or image data and, more recently, videos. Note that the text being analyzed by and created from generative AI tools encompasses an astonishing range of types, such as software code, protein sequences, music notation, mathematical expressions, and chemical formulas. …\n\n“Let’s start with the easy case. If you have a generation problem to solve, there’s only one game in town: generative AI. Depending on the sort of output you want to generate, you may need to use multimodal LLMs, like OpenAI’s GPT-4, Anthropic’s Claude 3.7 Sonnet, or Google’s Gemini 1.5; text-to-image models, like Dall-E; or special-purpose models that have been built for audio and other domains.\n\n“If you have a prediction problem, however, matters become more complicated.” Read the full article »\n\nRelated Articles\n\nAdditional Resources: Apply AI Lessons With Your Team\n\nMIT Sloan Management Review developed its Generative AI Strategy + Governance Toolkit with five of the world’s foremost experts on generative AI leadership: Ethan Mollick (University of Pennsylvania’s Wharton School), John Sviokla (Harvard Business School and GAI Insights), John K. Thompson (Hackett Group and the University of Michigan), George Westerman (MIT Sloan School of Management), and David A. Wood (Brigham Young University’s Marriott School of Business).\n\nThe kit includes a video lesson with Westerman, a GenAI strategy planner, a GenAI strategy checklist, a GenAI governance planner, articles, and more.",
      "summary": "Business strategy insights from MIT Sloan School of Management",
      "keywords": [
        "tools",
        "ai",
        "confusion",
        "cut",
        "genai",
        "work",
        "definitive",
        "leaders",
        "data",
        "reads",
        "article",
        "llms",
        "generative",
        "read"
      ]
    }
  ],
  "sources_used": [
    "MIT Sloan",
    "Hacker News"
  ]
}